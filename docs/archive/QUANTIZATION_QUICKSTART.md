# ğŸš€ å¿«é€Ÿé‡åŒ–æŒ‡å—

## ä¸€é”®é‡åŒ–ï¼ˆæœ€ç®€å•ï¼‰

### æ­¥éª¤ 1: é‡åŒ–æ¨¡å‹
```bash
# åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œ
cd /home/ec2-user/CosyVoice

python quantize_model.py \
  pretrained_models/Fun-CosyVoice3-0.5B-2512 \
  pretrained_models/Fun-CosyVoice3-0.5B-2512-quantized
```

### æ­¥éª¤ 2: ä¿®æ”¹æœåŠ¡é…ç½®
ç¼–è¾‘ `stream_service.py`:
```python
# ä¿®æ”¹ç¬¬ 18 è¡Œ
# åŸæ¥ï¼š
model_dir = "/home/ec2-user/CosyVoice/pretrained_models/Fun-CosyVoice3-0.5B-2512"

# æ”¹ä¸ºï¼š
model_dir = "/home/ec2-user/CosyVoice/pretrained_models/Fun-CosyVoice3-0.5B-2512-quantized"

# ä¿®æ”¹ç¬¬ 20 è¡Œï¼Œæ·»åŠ  fp16=True
# åŸæ¥ï¼š
cosyvoice = AutoModel(model_dir=model_dir)

# æ”¹ä¸ºï¼š
cosyvoice = AutoModel(model_dir=model_dir, fp16=True)
```

### æ­¥éª¤ 3: é‡å¯æœåŠ¡
```bash
# åœæ­¢æ—§æœåŠ¡
pkill -f stream_service.py

# å¯åŠ¨ä¼˜åŒ–åçš„æœåŠ¡
./start_fast.sh
```

---

## æ€§èƒ½æµ‹è¯•ï¼ˆå¯é€‰ï¼‰

```bash
# å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹çš„æ€§èƒ½
python benchmark_quantized.py \
  pretrained_models/Fun-CosyVoice3-0.5B-2512 \
  pretrained_models/Fun-CosyVoice3-0.5B-2512-quantized \
  "ä½ å¥½ï¼Œæ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„é¤å…ï¼Œä»Šå¤©æƒ³åƒç‚¹ä»€ä¹ˆå‘¢ï¼Ÿ"
```

---

## é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡ | åŸå§‹æ¨¡å‹ | é‡åŒ–æ¨¡å‹ | æå‡ |
|-----|---------|---------|-----|
| **æ¨¡å‹å¤§å°** | ~500 MB | ~250 MB | **2x** â¬‡ï¸ |
| **åŠ è½½æ—¶é—´** | ~10s | ~5s | **2x** â¬†ï¸ |
| **æ¨ç†é€Ÿåº¦** | 1.0x | 1.5-2.5x | **æœ€é«˜ 2.5x** â¬†ï¸ |
| **å†…å­˜å ç”¨** | ~2GB | ~1GB | **2x** â¬‡ï¸ |
| **éŸ³è´¨** | 100% | 95-98% | è½»å¾®ä¸‹é™ |

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: é‡åŒ–åæŠ¥é”™
```bash
# ç¡®ä¿ PyTorch ç‰ˆæœ¬ >= 1.13
pip install --upgrade torch

# é‡æ–°é‡åŒ–
python quantize_model.py <input> <output>
```

### é—®é¢˜ 2: éŸ³è´¨æ˜æ˜¾ä¸‹é™
```bash
# ä¸é‡åŒ– HiFi-GANï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
python quantize_model.py input output

# å¦‚æœå·²ç»é‡åŒ–äº†ï¼Œé‡æ–°é‡åŒ–ä½†è·³è¿‡ HiFi-GAN
python quantize_model.py input output  # é»˜è®¤å°±ä¼šè·³è¿‡
```

### é—®é¢˜ 3: GPU ä¸Šæ²¡æœ‰åŠ é€Ÿ
```bash
# ç¡®ä¿å¯ç”¨ FP16
cosyvoice = AutoModel(model_dir=model_dir, fp16=True)

# æ£€æŸ¥ CUDA ç‰ˆæœ¬
python -c "import torch; print(torch.cuda.is_available())"
```

### é—®é¢˜ 4: å†…å­˜ä¸è¶³
```bash
# é‡åŒ–å¯ä»¥å‡å°‘å†…å­˜å ç”¨
# å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œå‡å°‘ limit_concurrency
# åœ¨ stream_service.py ä¸­ä¿®æ”¹ï¼š
limit_concurrency=10  # ä» 20 æ”¹ä¸º 10
```

---

## é«˜çº§é€‰é¡¹

### åŒæ—¶é‡åŒ– HiFi-GANï¼ˆå¯èƒ½å½±å“éŸ³è´¨ï¼‰
```bash
python quantize_model.py \
  pretrained_models/original \
  pretrained_models/quantized \
  --quantize-hift
```

### ä»…ä½¿ç”¨ FP16ï¼ˆä¸é‡åŒ–ï¼‰
åœ¨ `stream_service.py` ä¸­ï¼š
```python
# ä¸éœ€è¦è¿è¡Œ quantize_model.py
# ç›´æ¥æ·»åŠ  fp16=True
cosyvoice = AutoModel(model_dir=model_dir, fp16=True)
```
è¿™æ ·å¯ä»¥è·å¾— **1.5-2x åŠ é€Ÿï¼Œå‡ ä¹æ— éŸ³è´¨æŸå¤±**ã€‚

---

## ç»„åˆä¼˜åŒ–ï¼ˆæœ€ä½³å®è·µï¼‰

```bash
# 1. é‡åŒ–æ¨¡å‹
python quantize_model.py pretrained_models/original pretrained_models/quantized

# 2. ä¿®æ”¹ stream_service.py
#    - model_dir æŒ‡å‘é‡åŒ–æ¨¡å‹
#    - æ·»åŠ  fp16=True
#    - å·²åŒ…å«å…¶ä»–ä¼˜åŒ–ï¼ˆç¼“å­˜ã€æ—¥å¿—ç­‰ï¼‰

# 3. ä½¿ç”¨é«˜æ€§èƒ½å¯åŠ¨è„šæœ¬
./start_fast.sh

# é¢„æœŸæ€»ä½“åŠ é€Ÿï¼š2-3x
```

---

## æœ€å¿«é…ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

```python
# stream_service.py å®Œæ•´ä¼˜åŒ–é…ç½®

# 1. ä½¿ç”¨é‡åŒ–æ¨¡å‹ + FP16
model_dir = "/home/ec2-user/CosyVoice/pretrained_models/Fun-CosyVoice3-0.5B-2512-quantized"
cosyvoice = AutoModel(model_dir=model_dir, fp16=True)

# 2. å¯ç”¨ CUDA ä¼˜åŒ–ï¼ˆåœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ï¼‰
import torch
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.set_float32_matmul_precision('high')

# 3. ä½¿ç”¨ uvloopï¼ˆé€šè¿‡ start_fast.shï¼‰
# 4. ä¼˜åŒ–å¹¶å‘é…ç½®
uvicorn.run(
    app,
    loop='uvloop',
    workers=1,
    limit_concurrency=20,
    backlog=2048,
)
```

---

## éªŒè¯é‡åŒ–æ•ˆæœ

### 1. æ£€æŸ¥æ¨¡å‹å¤§å°
```bash
du -h pretrained_models/Fun-CosyVoice3-0.5B-2512/*.pt
du -h pretrained_models/Fun-CosyVoice3-0.5B-2512-quantized/*.pt
```

### 2. æµ‹è¯•æ¨ç†é€Ÿåº¦
```bash
python benchmark_quantized.py original_model quantized_model
```

### 3. æµ‹è¯•éŸ³è´¨ï¼ˆä¸»è§‚è¯„ä»·ï¼‰
```bash
# ç”Ÿæˆæµ‹è¯•éŸ³é¢‘
python test_synthesize.py
```

---

## æ€»ç»“

âœ… **æ¨èæ–¹æ¡ˆ**ï¼ˆå¹³è¡¡æ€§èƒ½å’Œè´¨é‡ï¼‰ï¼š
```bash
1. python quantize_model.py <input> <output>  # é‡åŒ–æ¨¡å‹
2. ä¿®æ”¹ stream_service.py æ·»åŠ  fp16=True
3. ./start_fast.sh  # ä½¿ç”¨ä¼˜åŒ–å¯åŠ¨è„šæœ¬
```

âš¡ **æè‡´æ€§èƒ½**ï¼ˆå¯æ¥å—è½»å¾®è´¨é‡æŸå¤±ï¼‰ï¼š
```bash
1. é‡åŒ–æ¨¡å‹
2. fp16=True
3. é‡åŒ– HiFi-GAN (--quantize-hift)
4. ä½¿ç”¨ TensorRT (runtime/triton_trtllm/)
```

ğŸ’ **æœ€ä½³è´¨é‡**ï¼ˆä¼˜å…ˆéŸ³è´¨ï¼‰ï¼š
```bash
1. åªç”¨ fp16=Trueï¼ˆä¸é‡åŒ–ï¼‰
2. ä¸é‡åŒ– HiFi-GANï¼ˆé»˜è®¤ï¼‰
3. å…¶ä»–ä»£ç çº§ä¼˜åŒ–ï¼ˆå·²å®Œæˆï¼‰
```
