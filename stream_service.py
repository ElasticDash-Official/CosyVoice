# cosyvoice_service.py
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from cosyvoice.cli.cosyvoice import AutoModel, CosyVoice, CosyVoice2, CosyVoice3
from typing import Optional
import os
import logging
import tempfile
import soundfile as sf
import numpy as np
import io
import time
from functools import lru_cache

# é…ç½®æ—¥å¿— - ç”Ÿäº§ç¯å¢ƒä½¿ç”¨WARNINGçº§åˆ«å‡å°‘å¼€é”€
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# æŠ‘åˆ¶ç¬¬ä¸‰æ–¹åº“çš„è­¦å‘Š
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', message='.*weight_norm.*')

# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI()

# GPUä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
import torch
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True  # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å·ç§¯ç®—æ³•
    torch.set_float32_matmul_precision('high')  # ä½¿ç”¨TF32ç²¾åº¦åŠ é€Ÿ
    # å¯ç”¨ CUDA å›¾ä¼˜åŒ–ï¼ˆå‡å°‘kernelå¯åŠ¨å¼€é”€ï¼‰
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

# åˆå§‹åŒ– CosyVoice æ¨¡å‹
model_dir = "/home/ec2-user/CosyVoice/pretrained_models/CosyVoice2-0.5B-quantized"
alt_model_dir = "/home/ec2-user/CosyVoice/pretrained_models/CosyVoice2-0.5B-quantized-2"
# model_dir = "/home/ec2-user/CosyVoice/pretrained_models/Fun-CosyVoice3-0.5B-2512"
# alt_model_dir = "/home/ec2-user/CosyVoice/pretrained_models/Fun-CosyVoice3-0.5B-quantized"

# æ€§èƒ½ä¼˜åŒ–ï¼šæ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨ FP16 å’Œé‡åŒ–æ¨¡å‹
USE_FP16 = os.getenv('COSYVOICE_FP16', 'true').lower() == 'true'
USE_QUANTIZED = os.getenv('COSYVOICE_QUANTIZED', 'true').lower() == 'true'  # é»˜è®¤å¯ç”¨é‡åŒ–

if USE_QUANTIZED:
    # è‡ªåŠ¨æŸ¥æ‰¾é‡åŒ–æ¨¡å‹ç›®å½•ï¼ˆä¼˜å…ˆçº§ï¼šFP16 > åŸå§‹ï¼‰
    fp16_dir = model_dir.rstrip('/') + '-quantized'
    
    if os.path.exists(fp16_dir):
        model_dir = fp16_dir
        print(f"âœ… Using FP16 quantized model: {model_dir}")
    else:
        print(f"âš ï¸  Quantized model not found at {fp16_dir}")
        print(f"    Using original model (will be slower)")

print(f"ğŸ“‚ Loading model from: {model_dir}")
print(f"âš™ï¸  FP16 enabled: {USE_FP16}")
print(f"âš™ï¸  Quantized enabled: {USE_QUANTIZED}")

cosyvoice = AutoModel(model_dir=model_dir, load_trt=True, fp16=USE_FP16)

# è®°å½• worker ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—è¿½è¸ªï¼‰
import multiprocessing as mp
current_pid = os.getpid()

# ä½¿ç”¨æ›´å¯é çš„ worker è¯†åˆ«æ–¹æ³•ï¼šgunicorn è®¾ç½®çš„ç¯å¢ƒå˜é‡æˆ–è¿›ç¨‹åºå·
# gunicorn ä¼šä¸ºæ¯ä¸ª worker æŒ‰é¡ºåºåˆ†é…ï¼Œæˆ‘ä»¬é€šè¿‡ PID å–æ¨¡ + å¯åŠ¨é¡ºåºæ¥åŒºåˆ†
WORKER_ID = str(current_pid % 10)

# ä¸ºäº†å®ç°çœŸæ­£çš„è´Ÿè½½å‡è¡¡ï¼Œå¥‡æ•° worker ä½¿ç”¨ quantized-2 å‰¯æœ¬
if int(WORKER_ID) % 2 == 1:
    if os.path.exists(alt_model_dir) and alt_model_dir != model_dir:
        print(f"ğŸ”„ Worker {WORKER_ID} (PID: {current_pid}) loading alternate quantized model...")
        cosyvoice = AutoModel(model_dir=alt_model_dir, load_trt=True, fp16=USE_FP16)
        model_dir = alt_model_dir  # æ›´æ–° model_dir ç”¨äºæ—¥å¿—
        print(f"âœ… Worker {WORKER_ID} loaded: {alt_model_dir}")
    else:
        print(f"âš ï¸  Worker {WORKER_ID} (PID: {current_pid}): quantized-2 not found, using default")
        print(f"    Model: {model_dir}")
else:
    print(f"âœ… Worker {WORKER_ID} (PID: {current_pid}) using default: {model_dir}")

# æ£€æµ‹æ¨¡å‹ç±»å‹
model_type = type(cosyvoice).__name__
logger.warning(f"Loaded model type: {model_type}")

# å¯é€‰ï¼štorch.compileåŠ é€Ÿï¼ˆPyTorch 2.0+ï¼Œé¦–æ¬¡æ¨ç†ä¼šæ…¢ï¼Œåç»­ä¼šå¿«ï¼‰
USE_COMPILE = os.getenv('COSYVOICE_COMPILE', 'false').lower() == 'true'
if USE_COMPILE and hasattr(torch, 'compile'):
    try:
        logger.warning("Applying torch.compile optimization...")
        # ç¼–è¯‘æ¨¡å‹çš„å…³é”®éƒ¨åˆ†ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(cosyvoice, 'model'):
            cosyvoice.model.llm = torch.compile(cosyvoice.model.llm, mode='reduce-overhead')
        logger.warning("torch.compile applied successfully")
    except Exception as e:
        logger.warning(f"torch.compile failed: {e}")

# é»˜è®¤çš„promptéŸ³é¢‘æ–‡ä»¶è·¯å¾„å’Œå¯¹åº”æ–‡æœ¬
# ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿åœ¨ä»»ä½•å·¥ä½œç›®å½•ä¸‹éƒ½èƒ½æ‰¾åˆ°æ–‡ä»¶
# default_prompt_wav = "/home/ec2-user/CosyVoice/asset/zero_shot_prompt.wav"
# default_prompt_text = "å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚"  # zero_shot_prompt.wav çš„æ–‡å­—å†…å®¹

default_prompt_wav = "/home/ec2-user/CosyVoice/asset/paimon_prompt.wav"
default_prompt_text = "é‚£ç§åœ°æ–¹ä¸å¤ªå¯èƒ½ä¼šæœ‰äººå§ï¼Ÿ"  # paimon_prompt.wav çš„æ–‡å­—å†…å®¹

# é»˜è®¤çš„instruction(é¤é¦†åº—å‘˜åœºæ™¯)
default_instruction = "ä½ æ˜¯ä¸€ä½çƒ­æƒ…å‹å¥½çš„é¤é¦†åº—å‘˜,è¯´è¯æ¸©æŸ”äº²åˆ‡,è¯­æ°”ç¤¼è²Œä¸“ä¸šã€‚<|endofprompt|>"

# å¯¹äºCosyVoice(ç¬¬ä¸€ä»£),è·å–å¯ç”¨çš„speakersåˆ—è¡¨
available_speakers = []
default_speaker = None
if isinstance(cosyvoice, CosyVoice):
    available_speakers = cosyvoice.list_available_spks()
    logger.info(f"Available speakers: {available_speakers}")
    default_speaker = available_speakers[0] if available_speakers else None
    logger.info(f"Default speaker: {default_speaker}")

# æ€§èƒ½ä¼˜åŒ–ï¼šé¢„åŠ è½½å’Œç¼“å­˜é»˜è®¤éŸ³é¢‘
_default_audio_cache = None
_default_audio_info = None

if os.path.exists(default_prompt_wav):
    try:
        # é¢„åŠ è½½é»˜è®¤éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚éƒ½è¯»å–
        _default_audio_info = sf.info(default_prompt_wav)
        logger.info(f"âœ“ Preloaded default audio: {default_prompt_wav} ({_default_audio_info.duration:.2f}s)")
    except Exception as e:
        logger.warning(f"Failed to preload default audio: {e}")

# ç»Ÿä¸€çš„è¯·æ±‚æ¨¡å‹
class TTSRequest(BaseModel):
    text: str
    instruction: Optional[str] = None  # CosyVoice2/3ä½¿ç”¨,ä¾‹å¦‚: "ä½ æ˜¯ä¸€ä½çƒ­æƒ…å‹å¥½çš„é¤é¦†åº—å‘˜"
    speaker: Optional[str] = None  # CosyVoiceç¬¬ä¸€ä»£ä½¿ç”¨
    prompt_text: Optional[str] = None  # ç”¨äºzero-shotçš„å‚è€ƒæ–‡æœ¬

# ç»Ÿä¸€çš„åˆæˆè·¯ç”± - æ”¯æŒæ‰€æœ‰æ¨¡å‹
@app.post("/synthesize")
async def synthesize_streaming(
    text: str = Form(...),
    instruction: Optional[str] = Form(None),
    speaker: Optional[str] = Form(None),
    prompt_text: Optional[str] = Form(None),
    prompt_wav: Optional[UploadFile] = File(None)
):
    """
    ç»Ÿä¸€çš„è¯­éŸ³åˆæˆæ¥å£,è‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹:

    CosyVoiceç¬¬ä¸€ä»£:
    - ä½¿ç”¨ speaker å‚æ•° (å¦‚ "ä¸­æ–‡å¥³")

    CosyVoice2/3:
    - ä½¿ç”¨ instruction å‚æ•° (å¦‚ "ä½ æ˜¯ä¸€ä½çƒ­æƒ…å‹å¥½çš„å¥³é¤é¦†åº—å‘˜,è¯´è¯æ¸©æŸ”äº²åˆ‡,è¯­æ°”ç¤¼è²Œä¸“ä¸šã€‚<|endofprompt|>")
    - å¯é€‰: prompt_wav éŸ³é¢‘æ–‡ä»¶ (å¦‚æœä¸æä¾›,ä½¿ç”¨é»˜è®¤éŸ³é¢‘)
    - å¯é€‰: prompt_text å‚è€ƒæ–‡æœ¬

    é»˜è®¤ä¸ºé¤é¦†åº—å‘˜åœºæ™¯
    """
    try:
        # # CosyVoice ç¬¬ä¸€ä»£ - ä½¿ç”¨ speaker_id
        # if isinstance(cosyvoice, CosyVoice):
        #     speaker = speaker if speaker else default_speaker

        #     if speaker not in available_speakers:
        #         logger.warning(f"Requested speaker '{speaker}' not available. Using default: {default_speaker}")
        #         speaker = default_speaker

        #     if not speaker:
        #         raise HTTPException(status_code=500, detail="No available speakers found")

        #     logger.info(f"[CosyVoice] Synthesizing with speaker: {speaker}")

        #     async def audio_stream():
        #         for result in cosyvoice.inference_sft(text, speaker, stream=True):
        #             audio_chunk = result["tts_speech"].numpy().tobytes()
        #             yield audio_chunk

        #     return StreamingResponse(audio_stream(), media_type="application/octet-stream")

        # CosyVoice2/3 - æ”¯æŒå¤šç§æ¨ç†æ¨¡å¼
        if isinstance(cosyvoice, (CosyVoice2, CosyVoice3)):
            # æ¨ç†æ¨¡å¼é€‰æ‹©:
            # æœ‰ instruction â†’ ä½¿ç”¨ instruct2 (æŒ‡ä»¤æ§åˆ¶é£æ ¼)
            # æ—  instruction â†’ ä½¿ç”¨ zero_shot (çº¯å£°éŸ³å…‹éš†,éœ€è¦ prompt_text)
            # cross_lingual ç”¨äºç»†ç²’åº¦æ§åˆ¶ ([laughter], [breath] ç­‰æ ‡è®°)
            instruction_text = instruction if instruction else None

            # è®°å½•å¼€å§‹æ—¶é—´å’Œæ¨¡å‹ä¿¡æ¯
            start_ts = time.perf_counter()
            logger.warning(
                "[TTS] start worker=%s model=%s dir=%s text_len=%d instr=%s",
                WORKER_ID,
                model_type,
                model_dir,
                len(text),
                bool(instruction_text),
            )

            # å¤„ç†prompt_wavæ–‡ä»¶ (å¯é€‰) - ä¼˜åŒ–ç‰ˆæœ¬
            temp_wav_path = None
            if prompt_wav:
                # ç”¨æˆ·ä¸Šä¼ äº†éŸ³é¢‘æ–‡ä»¶
                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:
                    content = await prompt_wav.read()
                    temp_file.write(content)
                    temp_wav_path = temp_file.name
                logger.debug(f"Using uploaded audio: {temp_wav_path}")
            elif os.path.exists(default_prompt_wav):
                # ä½¿ç”¨é»˜è®¤éŸ³é¢‘æ–‡ä»¶ï¼ˆå·²é¢„åŠ è½½éªŒè¯ï¼‰
                temp_wav_path = default_prompt_wav
                logger.debug(f"Using default audio: {default_prompt_wav}")
            else:
                # æ²¡æœ‰éŸ³é¢‘æ–‡ä»¶
                logger.error(f"No audio file available: {default_prompt_wav}")
                raise HTTPException(
                    status_code=400,
                    detail=f"Voice reference required. Place file at: {default_prompt_wav}"
                )

            # æ ¹æ®å‚æ•°é€‰æ‹©æ¨ç†æ–¹æ³• - ä¼˜åŒ–ç‰ˆæœ¬
            if temp_wav_path:
                # è·³è¿‡é‡å¤éªŒè¯ï¼ˆä¸Šä¼ çš„æ–‡ä»¶å·²åœ¨ä¿å­˜æ—¶éªŒè¯ï¼Œé»˜è®¤æ–‡ä»¶å·²é¢„åŠ è½½ï¼‰
                # ä»…å¯¹ä¸Šä¼ æ–‡ä»¶è¿›è¡Œå¿«é€ŸéªŒè¯
                if prompt_wav:
                    try:
                        audio_info = sf.info(temp_wav_path)
                        if audio_info.duration < 0.1:  # å¿«é€Ÿæ£€æŸ¥
                            raise ValueError("Audio too short")
                    except Exception as e:
                        logger.error(f"Invalid audio: {e}")
                        raise HTTPException(status_code=400, detail="Invalid audio file")
                
                # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
                abs_wav_path = os.path.abspath(temp_wav_path)
                
                if instruction_text:
                    # INSTRUCT2 æ¨¡å¼: instruction + voice
                    logger.debug(f"Mode: INSTRUCT2, text_len={len(text)}")
                    inference_method = lambda: cosyvoice.inference_instruct2(
                        text,
                        instruction_text,
                        abs_wav_path, 
                        stream=True
                    )
                else:
                    # ZERO_SHOT æ¨¡å¼: çº¯å£°éŸ³å…‹éš†
                    actual_prompt_text = prompt_text if prompt_text else default_prompt_text
                    logger.debug(f"Mode: ZERO_SHOT, text_len={len(text)}")
                    inference_method = lambda: cosyvoice.inference_zero_shot(
                        text,
                        'You are a helpful assistant.<|endofprompt|>' + actual_prompt_text,
                        # actual_prompt_text,
                        abs_wav_path, 
                        stream=True
                    )
            else:
                raise HTTPException(
                    status_code=400,
                    detail="Voice reference required"
                )

            async def audio_stream():
                total_samples = 0
                sample_rate = getattr(cosyvoice, "sample_rate", 24000)
                
                try:
                    for result in inference_method():
                        # è·å–éŸ³é¢‘æ•°æ®ï¼ˆFloat32 æ ¼å¼ï¼‰
                        audio_chunk = result["tts_speech"].squeeze()

                        # è½¬æ¢ä¸º Int16 PCMï¼ˆæ ‡å‡†éŸ³é¢‘æ ¼å¼ï¼Œé¿å…æ‚éŸ³ï¼‰
                        # Float32 èŒƒå›´ [-1.0, 1.0] -> Int16 èŒƒå›´ [-32768, 32767]
                        audio_int16 = (audio_chunk * 32767).to(torch.int16)

                        # ç›´æ¥ä¼ è¾“ PCM æ•°æ®
                        yield audio_int16.cpu().numpy().tobytes()
                            
                finally:
                    # åªæ¸…ç†ä¸Šä¼ çš„ä¸´æ—¶æ–‡ä»¶
                    if prompt_wav and temp_wav_path and os.path.exists(temp_wav_path):
                        try:
                            os.unlink(temp_wav_path)
                        except:
                            pass

                    wall = time.perf_counter() - start_ts
                    audio_sec = total_samples / float(sample_rate) if total_samples else 0.0
                    rtf = wall / audio_sec if audio_sec > 0 else 0.0
                    logger.warning(
                        "[TTS] done worker=%s model=%s dir=%s wall=%.2fs audio=%.2fs rtf=%.3f text_len=%d",
                        WORKER_ID,
                        model_type,
                        model_dir,
                        wall,
                        audio_sec,
                        rtf,
                        len(text),
                    )

            return StreamingResponse(audio_stream(), media_type="audio/wav")

        else:
            raise HTTPException(status_code=500, detail=f"Unknown model type: {model_type}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during synthesis: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# éæµå¼æµ‹è¯•ç«¯ç‚¹ - è¿”å›å®Œæ•´ WAV æ–‡ä»¶
@app.post("/synthesize_complete")
async def synthesize_complete(
    text: str = Form(...),
    instruction: Optional[str] = Form(None),
    prompt_wav: Optional[UploadFile] = File(None)
):
    """
    éæµå¼åˆæˆæ¥å£ï¼Œè¿”å›å®Œæ•´çš„ WAV æ–‡ä»¶ï¼ˆç”¨äºæµ‹è¯•éŸ³è‰²ï¼‰
    """
    try:
        if isinstance(cosyvoice, (CosyVoice2, CosyVoice3)):
            instruction_text = instruction if instruction else None
            
            # å¤„ç†éŸ³é¢‘æ–‡ä»¶
            temp_wav_path = None
            if prompt_wav:
                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:
                    content = await prompt_wav.read()
                    temp_file.write(content)
                    temp_wav_path = temp_file.name
            elif os.path.exists(default_prompt_wav):
                temp_wav_path = default_prompt_wav
            else:
                raise HTTPException(status_code=400, detail="No voice reference audio")
            
            abs_wav_path = os.path.abspath(temp_wav_path)
            logger.debug(f"Complete synthesis - voice: {abs_wav_path}")
            
            # æ”¶é›†æ‰€æœ‰éŸ³é¢‘å—
            chunks = []
            
            if instruction_text:
                for result in cosyvoice.inference_instruct2(text, instruction_text, abs_wav_path, stream=False):
                    chunks.append(result["tts_speech"].squeeze().cpu().numpy())
            else:
                for result in cosyvoice.inference_zero_shot(
                    'æ”¶åˆ°å¥½å‹ä»è¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œé‚£ä»½æ„å¤–çš„æƒŠå–œä¸æ·±æ·±çš„ç¥ç¦è®©æˆ‘å¿ƒä¸­å……æ»¡äº†ç”œèœœçš„å¿«ä¹ï¼Œç¬‘å®¹å¦‚èŠ±å„¿èˆ¬ç»½æ”¾ã€‚',
                    'You are a helpful assistant.<|endofprompt|>å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚',
                    abs_wav_path,
                    stream=False
                ):
                    chunks.append(result["tts_speech"].squeeze().cpu().numpy())
            
            # æ‹¼æ¥å¹¶ç”Ÿæˆ WAV
            if chunks:
                full_audio = np.concatenate(chunks)
                buffer = io.BytesIO()
                sf.write(buffer, full_audio, cosyvoice.sample_rate, format='WAV')
                buffer.seek(0)
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if prompt_wav and temp_wav_path and os.path.exists(temp_wav_path):
                    os.unlink(temp_wav_path)
                
                return StreamingResponse(buffer, media_type="audio/wav")
            else:
                raise HTTPException(status_code=500, detail="No audio generated")
        else:
            raise HTTPException(status_code=400, detail="Model not supported")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during complete synthesis: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# è·å–æ¨¡å‹ä¿¡æ¯çš„è·¯ç”±
@app.get("/model_info")
async def get_model_info():
    info = {
        "model_type": model_type,
        "model_dir": model_dir,
    }

    if isinstance(cosyvoice, CosyVoice):
        info["speakers"] = available_speakers
        info["default_speaker"] = default_speaker
        info["usage"] = "Use /synthesize endpoint with 'speaker' parameter (CosyVoice v1)"
    else:
        info["default_instruction"] = default_instruction
        info["usage"] = "Use /synthesize endpoint with 'instruction' parameter (CosyVoice2/3)"

    info["endpoint"] = "/synthesize"

    return info

# è·å–å¯ç”¨speakersåˆ—è¡¨çš„è·¯ç”±(ä»…ç”¨äºCosyVoiceç¬¬ä¸€ä»£)
@app.get("/speakers")
async def get_speakers():
    if not isinstance(cosyvoice, CosyVoice):
        raise HTTPException(
            status_code=400,
            detail=f"Speakers list is only available for CosyVoice v1. Current model is {model_type}."
        )
    return {
        "speakers": available_speakers,
        "default_speaker": default_speaker
    }

# å¥åº·æ£€æŸ¥è·¯ç”±
@app.get("/health")
async def health_check():
    return {"status": "ok", "model_type": model_type}

if __name__ == "__main__":
    import uvicorn
    import multiprocessing
    
    # æ ¹æ®GPUæ•°é‡å’Œå†…å­˜è‡ªåŠ¨è°ƒæ•´workeræ•° (æ¯ä¸ªworkerä¼šåŠ è½½ä¸€æ¬¡æ¨¡å‹)
    # GPUå†…å­˜å……è¶³æ—¶å¯å¢åŠ åˆ°4-8ä¸ªworker
    num_workers = min(4, max(1, multiprocessing.cpu_count() // 2))
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=50000,
        workers=num_workers,    # å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†è¯·æ±‚
        limit_concurrency=50,   # å¢åŠ å¹¶å‘è¿æ¥æ•°
        timeout_keep_alive=30,  # 30ç§’keepaliveè¶…æ—¶
        backlog=2048,          # å¢åŠ è¿æ¥é˜Ÿåˆ—
        log_level="warning"     # åªè®°å½•è­¦å‘Šå’Œé”™è¯¯
    )
