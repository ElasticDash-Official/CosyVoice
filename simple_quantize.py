#!/usr/bin/env python3
"""
ÁÆÄÂåñÁâàÊ®°ÂûãÈáèÂåñÂ∑•ÂÖ∑ - Âè™ÈúÄË¶Å .pt ÊùÉÈáçÊñá‰ª∂
ÈÄÇÁî®‰∫éÂè™ÊúâÊùÉÈáçÊñá‰ª∂„ÄÅÊ≤°ÊúâÈÖçÁΩÆÊñá‰ª∂ÁöÑÊ®°Âûã
"""

import torch
import os
import sys
import shutil
from pathlib import Path

def get_file_size_mb(path):
    """Ëé∑ÂèñÊñá‰ª∂Â§ßÂ∞èÔºàMBÔºâ"""
    if os.path.exists(path):
        return os.path.getsize(path) / (1024 ** 2)
    return 0

def quantize_weight_file(input_path, output_path, verbose=True):
    """
    ÈáèÂåñÂçï‰∏™ÊùÉÈáçÊñá‰ª∂ÔºàFP32 -> FP16Ôºâ
    
    Args:
        input_path: ËæìÂÖ• .pt Êñá‰ª∂Ë∑ØÂæÑ
        output_path: ËæìÂá∫ .pt Êñá‰ª∂Ë∑ØÂæÑ
        verbose: ÊòØÂê¶ÊòæÁ§∫ËØ¶ÁªÜ‰ø°ÊÅØ
    """
    if verbose:
        print(f"  Loading: {os.path.basename(input_path)}...", end=' ')
    
    # Âä†ËΩΩÊùÉÈáç
    state_dict = torch.load(input_path, map_location='cpu')
    
    # ËΩ¨Êç¢‰∏∫ FP16
    quantized_dict = {}
    for key, value in state_dict.items():
        if isinstance(value, torch.Tensor) and value.dtype == torch.float32:
            quantized_dict[key] = value.half()
        else:
            quantized_dict[key] = value
    
    # ‰øùÂ≠ò
    torch.save(quantized_dict, output_path)
    
    if verbose:
        original_size = get_file_size_mb(input_path)
        quantized_size = get_file_size_mb(output_path)
        print(f"‚úì ({original_size:.1f}MB -> {quantized_size:.1f}MB, {original_size/quantized_size:.2f}x)")
    
    return get_file_size_mb(input_path), get_file_size_mb(output_path)

def simple_quantize(model_dir, output_dir, skip_hift=True):
    """
    ÁÆÄÂåñÁâàÈáèÂåñ - Áõ¥Êé•Â§ÑÁêÜ .pt Êñá‰ª∂
    
    Args:
        model_dir: ÂéüÂßãÊ®°ÂûãÁõÆÂΩï
        output_dir: ËæìÂá∫ÁõÆÂΩï
        skip_hift: ÊòØÂê¶Ë∑≥Ëøá HiFi-GAN ÈáèÂåñ
    """
    print(f"üîß Simple Quantization Tool")
    print(f"=" * 60)
    print(f"Input:  {model_dir}")
    print(f"Output: {output_dir}")
    print(f"=" * 60)
    
    # Êü•ÊâæÊâÄÊúâ .pt Êñá‰ª∂
    pt_files = []
    for file in os.listdir(model_dir):
        if file.endswith('.pt'):
            pt_files.append(file)
    
    if not pt_files:
        print(f"‚ùå No .pt files found in {model_dir}")
        return False
    
    print(f"\n‚úì Found {len(pt_files)} .pt file(s): {', '.join(pt_files)}")
    
    # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
    os.makedirs(output_dir, exist_ok=True)
    
    # ÈáèÂåñÊØè‰∏™Êñá‰ª∂
    print(f"\n‚ö° Quantizing models (FP32 -> FP16)...")
    total_original = 0
    total_quantized = 0
    
    for pt_file in pt_files:
        # Ê£ÄÊü•ÊòØÂê¶Ë∑≥Ëøá HiFi-GAN
        if skip_hift and ('hift' in pt_file.lower() or 'hifigan' in pt_file.lower()):
            print(f"  Copying: {pt_file}... ‚è≠Ô∏è  (skipped, preserving quality)")
            shutil.copy(f"{model_dir}/{pt_file}", f"{output_dir}/{pt_file}")
            size = get_file_size_mb(f"{model_dir}/{pt_file}")
            total_original += size
            total_quantized += size
        else:
            orig, quant = quantize_weight_file(
                f"{model_dir}/{pt_file}",
                f"{output_dir}/{pt_file}"
            )
            total_original += orig
            total_quantized += quant
    
    # Â§çÂà∂ÂÖ∂‰ªñÊñá‰ª∂
    print(f"\nüìã Copying other files...")
    other_files = [f for f in os.listdir(model_dir) 
                   if not f.endswith('.pt') and os.path.isfile(f"{model_dir}/{f}")]
    
    copied = 0
    for file in other_files:
        try:
            shutil.copy(f"{model_dir}/{file}", f"{output_dir}/{file}")
            copied += 1
        except:
            pass
    
    if copied > 0:
        print(f"  ‚úì Copied {copied} additional file(s)")
    else:
        print(f"  ‚ÑπÔ∏è  No additional files to copy")
    
    # ÂàõÂª∫Ê†áËÆ∞Êñá‰ª∂
    with open(f"{output_dir}/QUANTIZED_INFO.txt", 'w') as f:
        from datetime import datetime
        f.write("=" * 60 + "\n")
        f.write("CosyVoice Quantized Model (FP16)\n")
        f.write("=" * 60 + "\n")
        f.write(f"Original model: {model_dir}\n")
        f.write(f"Quantized on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"HiFi-GAN quantized: {not skip_hift}\n")
        f.write(f"Original size: {total_original:.1f} MB\n")
        f.write(f"Quantized size: {total_quantized:.1f} MB\n")
        f.write(f"Compression ratio: {total_original/total_quantized:.2f}x\n")
        f.write("\n")
        f.write("Usage:\n")
        f.write("  export COSYVOICE_FP16=true\n")
        f.write("  export COSYVOICE_QUANTIZED=true\n")
        f.write("  python stream_service.py\n")
    
    # ÊòæÁ§∫ÊÄªÁªì
    print(f"\n" + "=" * 60)
    print(f"üìä Quantization Summary")
    print(f"=" * 60)
    print(f"Original size:  {total_original:>10.1f} MB")
    print(f"Quantized size: {total_quantized:>10.1f} MB")
    print(f"Compression:    {total_original/total_quantized:>10.2f}x")
    print(f"Saved space:    {total_original-total_quantized:>10.1f} MB")
    print(f"=" * 60)
    
    print(f"\n‚úÖ Quantization complete!")
    print(f"üìÅ Output directory: {output_dir}")
    
    print(f"\nüí° To use the quantized model:")
    print(f"")
    print(f"   Option 1 - Auto-detect (Recommended):")
    print(f"      export COSYVOICE_FP16=true")
    print(f"      export COSYVOICE_QUANTIZED=true")
    print(f"      python stream_service.py")
    print(f"")
    print(f"   Option 2 - Direct path:")
    print(f"      # Edit stream_service.py:")
    print(f"      model_dir = '{output_dir}'")
    print(f"      cosyvoice = AutoModel(model_dir=model_dir, fp16=True)")
    print(f"")
    print(f"   Option 3 - High-performance:")
    print(f"      export COSYVOICE_QUANTIZED=true")
    print(f"      ./start_fast.sh")
    print(f"")
    
    return True

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Simple CosyVoice Model Quantization (FP32 -> FP16)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python simple_quantize.py pretrained_models/model pretrained_models/model-quantized
  
  # Also quantize HiFi-GAN (may affect quality)
  python simple_quantize.py pretrained_models/model pretrained_models/model-quantized --quantize-hift
        """
    )
    
    parser.add_argument('model_dir', type=str, help='Input model directory')
    parser.add_argument('output_dir', type=str, help='Output directory for quantized model')
    parser.add_argument('--quantize-hift', action='store_true',
                       help='Also quantize HiFi-GAN (may reduce audio quality)')
    
    args = parser.parse_args()
    
    # È™åËØÅËæìÂÖ•
    if not os.path.exists(args.model_dir):
        print(f"‚ùå Error: Model directory not found: {args.model_dir}")
        sys.exit(1)
    
    if not os.path.isdir(args.model_dir):
        print(f"‚ùå Error: Not a directory: {args.model_dir}")
        sys.exit(1)
    
    if os.path.exists(args.output_dir):
        response = input(f"‚ö†Ô∏è  Output directory exists: {args.output_dir}\n   Overwrite? (y/N): ")
        if response.lower() != 'y':
            print("Aborted.")
            sys.exit(0)
    
    # ÊâßË°åÈáèÂåñ
    success = simple_quantize(
        args.model_dir,
        args.output_dir,
        skip_hift=not args.quantize_hift
    )
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
